{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superasymmetry/ARC-AGI-Fun/blob/main/Potemkin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xAcOM0W5alY"
      },
      "source": [
        "# Potemkin Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avgzw_hE5YXK",
        "outputId": "206bd47f-6771-498d-aeea-7294a044effd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.6/504.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q trl accelerate bitsandbytes datasets sentencepiece jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pwl7Muh3UWiu"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y trl\n",
        "!pip install -U \"transformers>=4.42\" \"accelerate>=0.31\" peft\n",
        "!pip install -U git+https://github.com/huggingface/trl@main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1p7B8omAR4i"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "    print(HF_TOKEN)\n",
        "else:\n",
        "    print(\"Token is not set. Please save the token first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuO05Ma75i_p"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch, re\n",
        "\n",
        "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id, load_in_4bit=True, token=HF_TOKEN, device_map=\"auto\"\n",
        "# )\n",
        "\n",
        "\n",
        "model_id = \"Qwen/Qwen2.5-7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True             # Qwen models need this\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",                 # or {\"\": 0} for single‑GPU\n",
        "    load_in_4bit=True                  # <- correct kwarg\n",
        ")\n",
        "\n",
        "YES_RE  = re.compile(r\"\\b(yes|correct)\\b\", re.I)\n",
        "NO_RE   = re.compile(r\"\\b(no|incorrect|wrong)\\b\", re.I)\n",
        "\n",
        "# quick sanity check\n",
        "# model_inputs = tokenizer([\"An ABAB rhyme scheme\"], return_tensors=\"pt\").to(\"cuda\")\n",
        "# generated_ids = model.generate(**model_inputs)\n",
        "# tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVCSowpBSOHh"
      },
      "source": [
        "# Get Topics Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpgZhmKMSQXf"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import random\n",
        "\n",
        "\n",
        "class PromptStream(IterableDataset):\n",
        "    def __init__(self, path, tokenizer):\n",
        "        with jsonlines.open(path) as r:\n",
        "            self.prompts = [o[\"prompt\"] for o in r]\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"Qwen/Qwen2.5-7B\",\n",
        "                trust_remote_code=True             # Qwen models need this\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            random.shuffle(self.prompts)\n",
        "            for p in self.prompts:\n",
        "              if p is not None:\n",
        "                yield {\"prompt\": p}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5UDb3Pw3saH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = PromptStream('doing_tasks_dataset.jsonl', tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=32, num_workers=4, drop_last=True, timeout=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwKg3O_5_0lD"
      },
      "outputs": [],
      "source": [
        "print(dataset)\n",
        "print(dataset.__iter__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "031O0Dbv_B0g"
      },
      "outputs": [],
      "source": [
        "def generate(inputs, temperature):\n",
        "    # inputs = tokenizer(prompt=f\"Create an example of the concept/make a: {prompt}\", return_tensors=\"pt\").to(model.device)\n",
        "    answer_ids = model.generate(**inputs, temperature=temperature)\n",
        "    return tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def grade(execution, prompt, temperature=0, unclear_penalty=-0.2):\n",
        "    critique_prompt = (\n",
        "        f\"Here is the original task:\\n\\n{prompt}\\n\\n\"\n",
        "        f\"Here is a proposed answer:\\n{execution}\\n\\n\"\n",
        "        \"Does the proposed answer correctly and deeply satisfy the task? \"\n",
        "        \"Answer with either YES or NO and give one short reason.\"\n",
        "    )\n",
        "    inputs = tokenizer(critique_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        verdict_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            temperature=temperature\n",
        "        )\n",
        "    verdict = tokenizer.decode(verdict_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # 2‑c.  Parse verdict → reward\n",
        "    if YES_RE.search(verdict) and not NO_RE.search(verdict):\n",
        "        return 1.0\n",
        "    if NO_RE.search(verdict):\n",
        "        return -1.0\n",
        "    return unclear_penalty\n",
        "\n",
        "# def group_fn(prompts, model, tokenizer, device):\n",
        "#     groups, rewards = [], []\n",
        "#     for p in prompts:\n",
        "#         ans  = [generate(p,t) for t in (0.05,0.15,0.40,0.70,1.10)]\n",
        "#         rew  = [grade(a,p) for a in ans]\n",
        "#         # centre inside group\n",
        "#         r    = torch.tensor(rew)\n",
        "#         r    = (r - r.mean()) / (r.std() + 1e-7)\n",
        "#         groups.append(ans)\n",
        "#         rewards.append(r.tolist())\n",
        "#     return groups, rewards\n",
        "\n",
        "def group_fn(prompt_dicts, model, tokenizer, device):\n",
        "    groups, rewards = [], []\n",
        "    for pd in prompt_dicts:\n",
        "        p = pd[\"prompt\"]\n",
        "        print(p)\n",
        "        ans  = [generate(p,t) for t in (0.05,0.15,0.40,0.70,1.10)]\n",
        "        rew  = [grade(a,p) for a in ans]\n",
        "        r    = torch.tensor(rew)\n",
        "        r    = (r - r.mean()) / (r.std() + 1e-7)\n",
        "        groups.append(ans)\n",
        "        rewards.append(r.tolist())\n",
        "    return groups, rewards\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Dz5bQUSIE6"
      },
      "source": [
        "# GRPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K1_8LnU6ukiG",
        "outputId": "4e44a044-d8c7-4b5d-9051-ca984e4fb21c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trl[vllm] in /usr/local/lib/python3.11/dist-packages (0.20.0.dev0)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (1.9.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.53.2 in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (4.54.0)\n",
            "Requirement already satisfied: vllm>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (0.10.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (0.116.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (2.11.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (2.32.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from trl[vllm]) (0.35.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl[vllm]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl[vllm]) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl[vllm]) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl[vllm]) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl[vllm]) (2.7.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl[vllm]) (0.34.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl[vllm]) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl[vllm]) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl[vllm]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl[vllm]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl[vllm]) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl[vllm]) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl[vllm]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl[vllm]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl[vllm]) (2025.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->trl[vllm]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->trl[vllm]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->trl[vllm]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->trl[vllm]) (2025.7.14)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53.2->trl[vllm]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53.2->trl[vllm]) (0.21.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (5.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.2.0)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (1.0.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (9.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (5.29.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (3.12.14)\n",
            "Requirement already satisfied: openai<=1.90.0,>=1.87.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (1.90.0)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.22.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (11.3.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.9.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.10.11)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.7.30)\n",
            "Requirement already satisfied: outlines_core==0.2.10 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.2.10)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (5.6.3)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.21 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.1.21)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (4.14.1)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.2.1.1.post6)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (26.2.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.19.0)\n",
            "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.17.1)\n",
            "Requirement already satisfied: mistral_common>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (1.8.3)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (4.12.0.88)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.10.2 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.10.2)\n",
            "Requirement already satisfied: depyf==0.19.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.19.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (3.1.1)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (1.1.0)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (1.16.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (1.11.1.4)\n",
            "Requirement already satisfied: pybase64 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (1.4.2)\n",
            "Requirement already satisfied: cbor2 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (5.6.5)\n",
            "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.61.2)\n",
            "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.3->trl[vllm]) (2.48.0)\n",
            "Requirement already satisfied: torchaudio==2.7.1 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (2.7.1)\n",
            "Requirement already satisfied: torchvision==0.22.1 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.22.1)\n",
            "Requirement already satisfied: xformers==0.0.31 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.3->trl[vllm]) (0.0.31)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.19.0->vllm>=0.8.3->trl[vllm]) (0.8.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61.2->vllm>=0.8.3->trl[vllm]) (0.44.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (75.2.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->trl[vllm]) (0.47.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->trl[vllm]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->trl[vllm]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->trl[vllm]) (0.4.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->trl[vllm]) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->trl[vllm]) (0.16.0)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.0.8)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (2.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.3->trl[vllm]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.3->trl[vllm]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.3->trl[vllm]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.3->trl[vllm]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.3->trl[vllm]) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.3->trl[vllm]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.3->trl[vllm]) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl[vllm]) (1.1.5)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm>=0.8.3->trl[vllm]) (0.3.3)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (4.25.0)\n",
            "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.11/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (2.10.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.87.0->vllm>=0.8.3->trl[vllm]) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.87.0->vllm>=0.8.3->trl[vllm]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.87.0->vllm>=0.8.3->trl[vllm]) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.87.0->vllm>=0.8.3->trl[vllm]) (1.3.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.3->trl[vllm]) (1.1.1)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.3->trl[vllm]) (13.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl[vllm]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl[vllm]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl[vllm]) (2025.2)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.16.0)\n",
            "Requirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.14.9)\n",
            "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.1.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (1.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (0.26.0)\n",
            "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.11/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (24.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl[vllm]) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl[vllm]) (1.3.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.3->trl[vllm]) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (0.5.0.post1)\n",
            "Requirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.6.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (2.33.2)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm>=0.8.3->trl[vllm]) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.3->trl[vllm]) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install trl[vllm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSdxZfJKt4II"
      },
      "outputs": [],
      "source": [
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from datasets import IterableDataset\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True             # Qwen models need this\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",                 # or {\"\": 0} for single‑GPU\n",
        "    load_in_4bit=True                  # <- correct kwarg\n",
        ")\n",
        "cfg = GRPOConfig(output_dir=\"meta-llama/Llama-3.1-8B-Instruct\", learning_rate=2e-5, max_steps=2000, report_to=['tensorboard'])\n",
        "\n",
        "identity_collator = lambda batch: batch    # returns List[str] as‑is\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class= tokenizer,\n",
        "    train_dataset = PromptStream(\"doing_tasks_dataset.jsonl\", tokenizer),\n",
        "    reward_funcs=group_fn,         # or group_fn=… (see signature)\n",
        "    args = cfg          # contains max_steps\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brnkPo9pAVtP"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from peft import LoraConfig, get_peft_model\n",
        "prompts = [obj[\"prompt\"] for obj in jsonlines.open(\"doing_tasks_dataset.jsonl\")]\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\"\n",
        "MAX_STEPS = 2000\n",
        "KL_BETA = 0.02\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=64, lora_alpha=16, lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg).to(\"cuda\")\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16\n",
        ").eval()\n",
        "for p in ref_model.parameters(): p.requires_grad = False\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6WjsEtRCyaj",
        "outputId": "51e0d20f-b70f-4f49-f391-b7b2aa509d5c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def sequence_logprob(m, txt):\n",
        "    with torch.no_grad() if not m.training else torch.enable_grad():\n",
        "        tok = tokenizer(txt, return_tensors=\"pt\").to(DEVICE)\n",
        "        out = m(**tok)\n",
        "        # shift for next-token prediction\n",
        "        logits = out.logits[:, :-1]\n",
        "        labels = tok.input_ids[:, 1:]\n",
        "        lp = -torch.nn.functional.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            labels.reshape(-1),\n",
        "            reduction=\"sum\",\n",
        "        )\n",
        "    return lp\n",
        "\n",
        "\n",
        "YES_RE = re.compile(r\"\\b(yes|correct|right|true)\\b\", re.I)\n",
        "NO_RE  = re.compile(r\"\\b(no|incorrect|wrong|false)\\b\", re.I)\n",
        "\n",
        "def grade(answer, prompt, unclear_penalty=-0.2):\n",
        "    critique = (\n",
        "        f\"Task:\\n{prompt}\\n\\n\"\n",
        "        f\"Answer:\\n{answer}\\n\\n\"\n",
        "        \"Is the answer correct and deep? Reply YES or NO and one reason.\"\n",
        "    )\n",
        "    inp = tokenizer(critique, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        vid = model.generate(**inp, max_new_tokens=128, temperature=0.01, pad_token_id=tokenizer.eos_token_id)\n",
        "    verdict = tokenizer.decode(vid[0], skip_special_tokens=True)\n",
        "    if YES_RE.search(verdict) and not NO_RE.search(verdict):\n",
        "        return 1.0\n",
        "    if NO_RE.search(verdict):\n",
        "        return -1.0\n",
        "    return unclear_penalty\n",
        "\n",
        "def generate(prompt, temperature):\n",
        "    inp = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    out = model.generate(**inp, max_new_tokens=256, temperature=temperature)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "TEMPS = (0.05,0.15,0.40,0.70,1.10)\n",
        "for step in range(1, MAX_STEPS+1):\n",
        "    batch_prompts = random.sample(prompts, 4)\n",
        "\n",
        "    policy_loss = 0.0\n",
        "    kl_sum      = 0.0\n",
        "\n",
        "    for p in batch_prompts:\n",
        "        # k answers\n",
        "        answers = [generate(p, t) for t in TEMPS]\n",
        "        rewards = [grade(a, p)    for a in answers]\n",
        "\n",
        "        # centre rewards (GRPO advantage)\n",
        "        rew_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
        "        adv   = (rew_t - rew_t.mean()) / (rew_t.std() + 1e-7)\n",
        "\n",
        "        # accumulate loss for this prompt\n",
        "        for a, w in zip(answers, adv):\n",
        "            lp   = sequence_logprob(model, a)\n",
        "            rlp  = sequence_logprob(ref_model, a)\n",
        "            policy_loss += -w * lp\n",
        "            kl_sum      += (lp - rlp)\n",
        "\n",
        "    policy_loss /= (BATCH_SIZE * len(TEMPS))\n",
        "    kl_term      = KL_BETA * kl_sum / (BATCH_SIZE * len(TEMPS))\n",
        "    loss         = policy_loss + kl_term\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"[{step}/{MAX_STEPS}] loss={loss.item():.3f}  pol={policy_loss.item():.3f}  kl={kl_term.item():.3f}\")\n",
        "\n",
        "    if step % 500 == 0:\n",
        "        model.save_pretrained(f\"ckpt_step_{step}\")\n",
        "        tokenizer.save_pretrained(f\"ckpt_step_{step}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3gJfi3hYHJz"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "print(inspect.signature(GRPOTrainer.__init__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqJaHCB6IJWH"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import random\n",
        "\n",
        "class PromptStream(IterableDataset):\n",
        "    def __init__(self, path, tokenizer):\n",
        "        with jsonlines.open(path) as r:\n",
        "            self.prompts = [o[\"prompt\"] for o in r]\n",
        "        self.tok = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            random.shuffle(self.prompts)\n",
        "            for p in self.prompts:\n",
        "                enc = self.tok(p, return_tensors=\"pt\", truncation=True, padding=False)\n",
        "                if not(enc==None):\n",
        "                  # Drop batch dim; ONLY tensors – no strings\n",
        "                  yield {\n",
        "                      \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "                      \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "                  }\n",
        "\n",
        "dataset = PromptStream('doing_tasks_dataset.jsonl', tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=32, num_workers=4, drop_last=True, timeout=60)\n",
        "dataset.__iter__()\n",
        "\n",
        "def group_fn(batch, model, tokenizer, device):\n",
        "    prompts = tokenizer.batch_decode(\n",
        "        [b[\"input_ids\"] for b in batch],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    answer_groups, reward_groups = [], []\n",
        "    for p in prompts:\n",
        "        answers = [generate(p, t) for t in (0.05, 0.15, 0.40, 0.70, 1.10)]\n",
        "        raw     = [grade(a, p)    for a in answers]\n",
        "        adv     = torch.tensor(raw, dtype=torch.float32, device=device)\n",
        "        adv     = (adv - adv.mean()) / (adv.std() + 1e-7)\n",
        "        answer_groups.append(answers)\n",
        "        reward_groups.append(adv)\n",
        "    return answer_groups, reward_groups\n",
        "\n",
        "cfg = GRPOConfig(output_dir=\"Qwen2-0.5B-GRPO\", learning_rate=2e-5, max_steps=2000)\n",
        "\n",
        "stream = PromptStream(\"doing_tasks_dataset.jsonl\", tokenizer)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model            = model,\n",
        "    reward_funcs     = group_fn,       # matches your signature\n",
        "    args             = cfg,\n",
        "    train_dataset    = stream,\n",
        "    processing_class = tokenizer\n",
        ")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKw7x_OFH72V"
      },
      "source": [
        "# Direct Preference Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kfoZsbBFqxT"
      },
      "outputs": [],
      "source": [
        "ans1, _, r1 = ask_and_verify(prompt, temperature=0.7)\n",
        "ans2, _, r2 = ask_and_verify(prompt, temperature=1.1)\n",
        "chosen, rejected = (ans1, ans2) if r1 > r2 else (ans2, ans1)\n",
        "from trl import DPOTrainer\n",
        "dataset = [{\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected} for ...]\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,           # uses frozen copy internally\n",
        "    args=training_args,\n",
        "    beta=0.1,                 # softness of preference\n",
        "    train_dataset=dataset\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "467BdwEqSKuQ"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.optim import AdamW\n",
        "\n",
        "lora = LoraConfig(r=64, target_modules=[\"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\"])\n",
        "model = get_peft_model(model, lora)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "KL_BETA   = 0.02\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B\", load_in_4bit=True, device_map=\"cuda\")\n",
        "ref_model.eval()\n",
        "ref_model.requires_grad_(False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOHFKoHDBfwAc42o9ffNgRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}