{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9Y017Uv0cRb4e3ibRESE0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superasymmetry/AGI-Fun/blob/main/potemkintraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1SzD_KlW1Du"
      },
      "outputs": [],
      "source": [
        "!pip install -q trl accelerate bitsandbytes datasets sentencepiece jsonlines\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch, re\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=torch.float32\n",
        ")\n",
        "\n",
        "model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "YES_RE  = re.compile(r\"\\b(yes|correct)\\b\", re.I)\n",
        "NO_RE   = re.compile(r\"\\b(no|incorrect|wrong)\\b\", re.I)\n",
        "\n",
        "# quick sanity check\n",
        "# model_inputs = tokenizer([\"An ABAB rhyme scheme\"], return_tensors=\"pt\").to(\"cuda\")\n",
        "# generated_ids = model.generate(**model_inputs)\n",
        "# tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "Gm2PJJObW7vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from datasets import IterableDataset\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True             # Qwen models need this\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",                 # or {\"\": 0} for single‑GPU\n",
        "    load_in_4bit=True                  # <- correct kwarg\n",
        ")\n",
        "cfg = GRPOConfig(output_dir=\"meta-llama/Llama-3.1-8B-Instruct\", learning_rate=2e-5, max_steps=2000, report_to=['tensorboard'])\n",
        "\n",
        "identity_collator = lambda batch: batch    # returns List[str] as‑is\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class= tokenizer,\n",
        "    train_dataset = PromptStream(\"doing_tasks_dataset.jsonl\", tokenizer),\n",
        "    reward_funcs=group_fn,         # or group_fn=… (see signature)\n",
        "    args = cfg          # contains max_steps\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "IwTLdIUsW__0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from peft import LoraConfig, get_peft_model\n",
        "prompts = [obj[\"prompt\"] for obj in jsonlines.open(\"doing_tasks_dataset.jsonl\")]\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=64, lora_alpha=16, lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg).to(\"cuda\")\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2-0.5B-Instruct\", device_map=\"auto\",\n",
        "    # torch_dtype=torch.bfloat16\n",
        ").eval()\n",
        "for p in ref_model.parameters(): p.requires_grad = False\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "aapqq-h-XDBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\"\n",
        "MAX_STEPS = 200\n",
        "KL_BETA = 0.02\n",
        "\n",
        "def sequence_logprob(m, txt):\n",
        "    with torch.no_grad() if not m.training else torch.enable_grad():\n",
        "        tok = tokenizer(txt, return_tensors=\"pt\").to(DEVICE)\n",
        "        out = m(**tok)\n",
        "        # shift for next-token prediction\n",
        "        logits = out.logits[:, :-1]\n",
        "        labels = tok.input_ids[:, 1:]\n",
        "        lp = -torch.nn.functional.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            labels.reshape(-1),\n",
        "            reduction=\"sum\",\n",
        "        )\n",
        "    return lp\n",
        "\n",
        "\n",
        "YES_RE = re.compile(r\"\\b(yes|correct|right|true)\\b\", re.I)\n",
        "NO_RE  = re.compile(r\"\\b(no|incorrect|wrong|false)\\b\", re.I)\n",
        "\n",
        "def grade(answer, prompt, unclear_penalty=-0.2):\n",
        "    critique = (\n",
        "        f\"Task:\\n{prompt}\\n\\n\"\n",
        "        f\"Answer:\\n{answer}\\n\\n\"\n",
        "        \"Is the answer correct and deep? Reply YES or NO and one reason.\"\n",
        "    )\n",
        "    inp = tokenizer(critique, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        vid = model.generate(**inp, max_new_tokens=128, temperature=0.01, pad_token_id=tokenizer.eos_token_id)\n",
        "    verdict = tokenizer.decode(vid[0], skip_special_tokens=True)\n",
        "    if YES_RE.search(verdict) and not NO_RE.search(verdict):\n",
        "        return 1.0\n",
        "    if NO_RE.search(verdict):\n",
        "        return -1.0\n",
        "    return unclear_penalty\n",
        "\n",
        "def generate(prompt, temperature):\n",
        "    inp = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    out = model.generate(**inp, max_new_tokens=256, temperature=temperature)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "TEMPS = (0.05,0.15,0.40,0.70,1.10)\n",
        "for step in range(1, MAX_STEPS+1):\n",
        "    batch_prompts = random.sample(prompts, 4)\n",
        "\n",
        "    policy_loss = 0.0\n",
        "    kl_sum      = 0.0\n",
        "\n",
        "    for p in batch_prompts:\n",
        "        # k answers\n",
        "        answers = [generate(p, t) for t in TEMPS]\n",
        "        rewards = [grade(a, p)    for a in answers]\n",
        "\n",
        "        # centre rewards (GRPO advantage)\n",
        "        rew_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
        "        adv   = (rew_t - rew_t.mean()) / (rew_t.std() + 1e-7)\n",
        "\n",
        "        # accumulate loss for this prompt\n",
        "        for a, w in zip(answers, adv):\n",
        "            lp   = sequence_logprob(model, a)\n",
        "            rlp  = sequence_logprob(ref_model, a)\n",
        "            policy_loss += -w * lp\n",
        "            kl_sum      += (lp - rlp)\n",
        "\n",
        "    policy_loss /= (BATCH_SIZE * len(TEMPS))\n",
        "    kl_term      = KL_BETA * kl_sum / (BATCH_SIZE * len(TEMPS))\n",
        "    loss         = policy_loss + kl_term\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    print(f\"[{step}/{MAX_STEPS}] loss={loss.item():.3f}  pol={policy_loss.item():.3f}  kl={kl_term.item():.3f}\")\n",
        "\n",
        "    if step % 500 == 0:\n",
        "        model.save_pretrained(f\"ckpt_step_{step}\")\n",
        "        tokenizer.save_pretrained(f\"ckpt_step_{step}\")"
      ],
      "metadata": {
        "id": "3Ai9rUdlXEmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "    print(HF_TOKEN)\n",
        "else:\n",
        "    print(\"Token is not set. Please save the token first.\")\n",
        "\n",
        "# Push the LoRA adapters\n",
        "model.push_to_hub(\"stringbot/potemkin\")\n"
      ],
      "metadata": {
        "id": "COGl2CD0XF8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}